{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2C9h5qC9qDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElWcBGG0FGxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 간단하게 처리하는 방법\n",
        "# from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b8Urt7aEfQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "imdb_data = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "# pos, neg 값을 숫자로 변환\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].replace('positive', 1)\n",
        "# imdb_data['sentiment'].replace('positive', 1, inplace=True)\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].replace('negative', 0)\n",
        "# 정규표현식을 써서, 단어가 아니면 공백으로\n",
        "imdb_data['review'] = imdb_data['review'].str.replace(\"[^\\w]|br\", \" \")\n",
        "# 혹시 공백이 있으면 제거\n",
        "imdb_data['review'] = imdb_data['review'].replace(\"\", np.nan)\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].replace(\"\", np.nan)\n",
        "# null array 없애는 함수\n",
        "imdb_data = imdb_data.dropna(how='any')\n",
        "\n",
        "print(\"# preprocessing done\")\n",
        "\n",
        "review_train, review_test, y_train, y_test = train_test_split(imdb_data['review'], imdb_data['sentiment'], test_size=0.25, shuffle=False, random_state=23)\n",
        "\n",
        "print(\"# split done\")\n",
        "\n",
        "stopwords = ['a', 'an']\n",
        "\n",
        "X_train = []\n",
        "for stc in review_train:\n",
        "    token = []\n",
        "    words = stc.split()\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            token.append(word)\n",
        "    X_train.append(token)\n",
        "\n",
        "X_test = []\n",
        "for stc in review_test:\n",
        "    token = []\n",
        "    words = stc.split()\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            token.append(word)\n",
        "    X_test.append(token)\n",
        "\n",
        "print(\"# tokenization done\")\n",
        "\n",
        "# 왜 트레인셋만?.. 원하신다면 처음에 test/train 스플릿하기 전에, 전처리하셔서 fit하셔도\n",
        "# 왜 5000?.. 유의미한 단어 갯수를 생각해보자! 빈도수 1개다 -> 버리자, 빈도수 2개다 -> 버리자\n",
        "tokenizer = Tokenizer(5000)\n",
        "# train 을 기준으로 단어마다의 인덱스를 부여\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# 부여된 정수 인덱스로 변환\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print(\"# int_encoding done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb7jiQeYX3lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train[1])\n",
        "print(y_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd6GpxL3FiuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문장 길이를 맞춰준다\n",
        "# 임의로 맞추는게 아니고, 데이터셋을 보면서 최대 문장의 길이가 얼마인지 확인하시고 거기에 맞춰서\n",
        "# imdb일때는 500, 네이버일때는 50으로 맞췄었습니다!\n",
        "max_len = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvJuOr7aGXI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BSOtN80GiTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 레이어들을 쌓을 모델을 생성\n",
        "model = Sequential()\n",
        "# 단어를 임베딩하는데, 5000개 (imdb) 혹은 20000개 (네이버) 의 단어를 120차원으로 내보내겠다\n",
        "# 1인자 = 내가 넣을 단어의 갯수 (총 인덱스의 갯수), 2인자 = 출력할 차원 (덴스 벡터의 차원), 3인자 = 문장의 길이\n",
        "model.add(Embedding(5000, 120))\n",
        "# RNN - simpleRNN / LSTM\n",
        "model.add(LSTM(120))\n",
        "# 긍정/부정을 판단하니까 이진 분류 -> sigmoid 함수 사용\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P761nRyqHLUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 혹시 5회 이상 검증데이터 loss가 증가하면, 과적합될 수 있으므로 학습을 조기종료!\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "# 훈련을 거듭하면서, 가장 검증데이터 정확도가 높았던 순간을 체크포인트로 저장\n",
        "model_check = ModelCheckpoint('the_best.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29WEfh8GH3UH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 긍정/부정을 판단하니까 손실함수는 이진 교차 엔트로피, 최적화는 adam, 평가 기준은 acc (출력할때 뜬다)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=64, callbacks=[early_stop, model_check])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx-KCvH9sLQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 정확도 측정\n",
        "# 출력하면 [loss, acc]\n",
        "print(model.evaluate(X_test, y_test))\n",
        "# X 값은 전처리, 토큰화, 정수인코딩이 된 상태의 문장이어야\n",
        "print(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x6J_KHmnpdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc1mPNHNb79u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## 한글 전처리부분입니다 ########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "naver_data = pd.read_table(\"./drive/My Drive/ratings.txt\", encoding='utf-8')\n",
        "# 단어 아니면 삭제\n",
        "naver_data['document'] = naver_data['document'].str.replace(\"[^\\w]\", \" \")\n",
        "# 혹시 공백이 있으면 null array로\n",
        "naver_data['document'] = naver_data['document'].replace('', np.nan)\n",
        "naver_data['label'] = naver_data['label'].replace('', np.nan)\n",
        "# null array 모두 제거 (공백인 열 모두 제거)\n",
        "naver_data = naver_data.dropna(how='any')\n",
        "\n",
        "print(\"# preproecssing done\")\n",
        "\n",
        "# test/train 스플릿하고\n",
        "review_train, review_test, y_train, y_test = train_test_split(naver_data['document'], naver_data['label'], test_size=0.25, shuffle=False, random_state=1004)\n",
        "\n",
        "print('# split done')\n",
        "\n",
        "# stopwords 지정\n",
        "stopwords = ['를', '을', '이', '가', '으로', '로', '에', '에서']\n",
        "\n",
        "# 토큰화 진행\n",
        "X_train = []\n",
        "for stc in review_train:\n",
        "    token = []\n",
        "    words = Okt().morphs(stc, stem=True)\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            token.append(word)\n",
        "    X_train.append(token)\n",
        "\n",
        "X_test = []\n",
        "for stc in review_test:\n",
        "    token = []\n",
        "    words = Okt().morphs(stc, stem=True)\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            token.append(word)\n",
        "    X_test.append(token)\n",
        "\n",
        "print('# tokenization done')\n",
        "\n",
        "# X_train 단어들을 토대로 정수 인덱스 설정, 전체 단어 갯수 설정\n",
        "# 유의미한 단어? 빈도수가 1~2개인 단어 버려도 큰 영향을 끼치지 않을것 -> count함수써서 빈도수 낮은 것들을 버리고, 남은 단어의 갯수들\n",
        "tokenizer = Tokenizer(20000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# 설정된 정수 인덱스를 토대로 변환\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print('# int_encoding done')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}