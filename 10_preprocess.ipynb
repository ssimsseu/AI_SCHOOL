{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAbpqiwpsnp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "imdb_data = pd.read_csv(\"IMDBDataset.csv\")\n",
        "# pos, neg 값을 숫자로 변형\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].replace(\"positive\", 1)\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].replace(\"negative\", 0)\n",
        "# 단어 아니면 삭제\n",
        "imdb_data['review'] = imdb_data['review'].str.replace(\"[^\\w]|br\", \" \")\n",
        "# 혹시 공백이 있으면 null array로\n",
        "imdb_data['review'] = imdb_data['review'].replace('', np.nan)\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].replace('', np.nan)\n",
        "# null array 모두 제거 (공백인 열 모두 제거)\n",
        "imdb_data = imdb_data.dropna(how='any')\n",
        "\n",
        "print(\"# preproecssing done\")\n",
        "\n",
        "# test/train 스플릿하고\n",
        "review_train, review_test, y_train, y_test = train_test_split(imdb_data['review'], imdb_data['sentiment'], test_size=0.4, shuffle=False, random_state=1004)\n",
        "\n",
        "print('# split done')\n",
        "\n",
        "# stopwords 지정\n",
        "stopwords = ['a', 'an']\n",
        "\n",
        "# 토큰화 진행\n",
        "X_train = []\n",
        "for stc in review_train:\n",
        "    token = []\n",
        "    words = stc.split()\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            token.append(word)\n",
        "    X_train.append(token)\n",
        "\n",
        "X_test = []\n",
        "for stc in review_test:\n",
        "    token = []\n",
        "    words = stc.split()\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            token.append(word)\n",
        "    X_test.append(token)\n",
        "\n",
        "print('# tokenization done')\n",
        "\n",
        "# X_train 단어들을 토대로 정수 인덱스 설정, 전체 단어 갯수 설정\n",
        "tokenizer = Tokenizer(5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# 설정된 정수 인덱스를 토대로 변환\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print('# int_encoding done')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}