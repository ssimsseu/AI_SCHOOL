{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV-f3kHg_tKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install glove_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weH0OcqhIpmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def open_csv():\n",
        "    # csv파일을 연다!\n",
        "    f = open('IMDBDataset.csv', 'r', encoding='utf-8')\n",
        "    csvreader = csv.reader(f)\n",
        "    \n",
        "    doc_list = []\n",
        "\n",
        "    next(csvreader)\n",
        "    for f in csvreader:\n",
        "        line = re.compile(\"[^\\w]\").sub(' ', f[0].lower())\n",
        "        doc_list.append(line.split())\n",
        "\n",
        "    return doc_list\n",
        "\n",
        "doc_list = open_csv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsjyTkLRIhvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus()\n",
        "# 같이 등장한다는 것의 기준을 5로 지정\n",
        "corpus.fit(doc_list, window=5)\n",
        "\n",
        "# 경사하강법을 사용하기 때문에 학습률을 설정, 아웃풋 벡터의 차원은 100\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "# 쓰레드 개수는 4개, 훈련 횟수는 20번, verbose 설명\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "# 유사도 검색을 위해서 행렬의 index 정보를 입력\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGGIFxdWKonp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_result = glove.most_similar('dog')\n",
        "print(glove_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l50Ul8FQOlsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "token = Tokenizer()\n",
        "text = \"Tensorflow tokenizer is good\"\n",
        "\n",
        "# 리스트 형태로 넣어주어야 제대로 토큰화가 된다 (아니면 한글자씩 짜름)\n",
        "# fit 한 것을 토대로 추후에 sequence 를 생성한다\n",
        "token.fit_on_texts([text])\n",
        "\n",
        "# 단어 집합\n",
        "print(token.word_index)\n",
        "# 단어 빈도\n",
        "print(token.word_counts)\n",
        "\n",
        "# 정수 인코딩\n",
        "text_2 = \"Tensorflow tokenizer is not good good\"\n",
        "int_encoding = token.texts_to_sequences([text_2])\n",
        "print(int_encoding)\n",
        "\n",
        "# 행렬화\n",
        "text_list = ['안녕하세요 감사해요 잘있어요 다시 만나요', '오늘도 기분 좋은 하루', '안녕하세요 오늘도 감사해요']\n",
        "token.fit_on_texts(text_list)\n",
        "\n",
        "# count 는 dtm, binary 는 존재 여부, freq 는 빈도율, tfidf 는 tfidf\n",
        "matrix = token.texts_to_matrix(text_list, mode='tfidf')\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}